{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/MLOpsAIKB/blob/main/Building_MLOps_AI_Body_of_Knowledge_Part_4_Query_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# MLOps AI Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 4, query the vector database using ChatGPT\n",
        "\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This does not require a GPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "JSzQ8Ud4WeiH"
      },
      "id": "JSzQ8Ud4WeiH"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8Uynu3LfWYst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083fbbc4-c5e7-40b0-c559-a674625fb4ca"
      },
      "id": "8Uynu3LfWYst",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "KB_FOLDER = \"/content/gdrive/MyDrive/MLOpsKB\"  # Google drive folder to save the knowledgebase\n",
        "YT_DATASTORE = os.path.join(KB_FOLDER, \"youtube/datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "YT_AUDIO_FOLDER = os.path.join(KB_FOLDER, \"youtube/audio\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_FOLDER = os.path.join(YT_AUDIO_FOLDER, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "TRANSCRIPTS_TEXT_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"text\")  # Sub-directory for text of audio files\n",
        "TRANSCRIPTS_WHISPER_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(KB_FOLDER):\n",
        "    os.makedirs(KB_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(YT_DATASTORE):\n",
        "    os.makedirs(YT_DATASTORE)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(YT_AUDIO_FOLDER):\n",
        "    os.makedirs(YT_AUDIO_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_TEXT_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_TEXT_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_WHISPER_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_WHISPER_FOLDER)"
      ],
      "metadata": {
        "id": "vWeQuyzjU0m_"
      },
      "id": "vWeQuyzjU0m_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03__ie72B8J_"
      },
      "source": [
        "Use Pinecone or FAISS for the Vector Database"
      ],
      "id": "03__ie72B8J_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6TkUqCJB_2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f57b60-102e-4c20-82e0-c0760406c4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q tiktoken"
      ],
      "id": "W6TkUqCJB_2X"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = 'FAISS' # Set to 'Pinecone' or 'FAISS' for the vector datbase"
      ],
      "metadata": {
        "id": "cckWJ2E2TAjK"
      },
      "id": "cckWJ2E2TAjK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    !pip install -q pinecone-client\n",
        "    from langchain.vectorstores import Pinecone\n",
        "    from tqdm.autonotebook import tqdm\n",
        "    import pinecone\n",
        "\n",
        "    # initialize pinecone\n",
        "    pinecone.init(\n",
        "        api_key=\"\",  # find at app.pinecone.io\n",
        "        environment=\"us-west4-gcp-free\"  # next to api key in console\n",
        "        )\n",
        "\n",
        "    index_name = \"knowledge\" # Put your Pincecone index name here\n",
        "    name_space = \"mlopskb\" # Put your Pincecone namespace here\n",
        "\n",
        "else:\n",
        "    !pip install -q faiss-cpu\n",
        "    from langchain.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "YoCtnnwMS9V9"
      },
      "id": "YoCtnnwMS9V9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUbD-eXCXNy-"
      },
      "source": [
        "Set up OPEN_API_KEY and necessary variables"
      ],
      "id": "TUbD-eXCXNy-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY7CJZoh5cma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # Add your OpenAI API Key here\n",
        "\n",
        "#MODEL = \"gpt-3\"\n",
        "#MODEL = \"gpt-3.5-turbo\"\n",
        "#MODEL = \"gpt-3.5-turbo-0613\"\n",
        "#MODEL = \"gpt-3.5-turbo-16k\"\n",
        "MODEL = \"gpt-3.5-turbo-16k-0613\"\n",
        "#MODEL = \"gpt-4\"\n",
        "#MODEL = \"gpt-4-0613\"\n",
        "#MODEL = \"gpt-4-32k-0613\""
      ],
      "id": "tY7CJZoh5cma"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "# Query using the vector store with ChatGPT integration"
      ],
      "id": "35f99145"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup access to the Pinecone or FAISS vector database"
      ],
      "metadata": {
        "id": "S5Vf831zQLJa"
      },
      "id": "S5Vf831zQLJa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibz6Tz_7iPeQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "id": "ibz6Tz_7iPeQ"
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    vector_store = Pinecone.from_existing_index(index_name, embeddings, namespace=name_space)\n",
        "\n",
        "else:\n",
        "    # Open datastore\n",
        "    from langchain.vectorstores import FAISS\n",
        "    if os.path.exists(f\"{YT_DATASTORE}\"):\n",
        "        vector_store = FAISS.load_local(\n",
        "            f\"{YT_DATASTORE}\",\n",
        "            OpenAIEmbeddings()\n",
        "            )\n",
        "    else:\n",
        "        print(f\"Missing files. Upload index.faiss and index.pkl files to data_store directory first\")\n"
      ],
      "metadata": {
        "id": "QQDKy5IRT98I"
      },
      "id": "QQDKy5IRT98I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the prompt"
      ],
      "metadata": {
        "id": "WDcuSm-wQPC-"
      },
      "id": "WDcuSm-wQPC-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"\n",
        "    You are MLOpsGPT a mlops specialist bot.\n",
        "    You use examples from MLOps in your answers.\n",
        "    Your language should be for an 12 year old to understand.\n",
        "    If you do not know the answer to a question, do not make information up - instead, ask a follow-up question in order to gain more context.\n",
        "    Use a mix of technical and colloquial uk english language to create an accessible and engaging tone.\n",
        "    Use the following pieces of context to answer the users question.\n",
        "    Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
        "----------------\n",
        "{summaries}\n",
        "\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "id": "32a49412"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the LLM API"
      ],
      "metadata": {
        "id": "jlIfoZccQRmm"
      },
      "id": "jlIfoZccQRmm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3018f865"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "llm = ChatOpenAI(model_name=MODEL, temperature=0)  # Modify model_name if you have access to GPT-4\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "id": "3018f865"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "source": [
        "#### Use the chain to query"
      ],
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the sources so we can find the YouTube videos"
      ],
      "metadata": {
        "id": "6_aZT0JXPr_9"
      },
      "id": "6_aZT0JXPr_9"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I learn about MLOps?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "dRFI-plXIlPY"
      },
      "id": "dRFI-plXIlPY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1IKUV4IuZQ",
        "outputId": "f25c64de-9211-4d6f-8b29-06cf85c73088"
      },
      "id": "8b1IKUV4IuZQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I learn about MLOps?\n",
            "If you're interested in learning about MLOps, there are several ways you can get started. Here are a few suggestions:\n",
            "\n",
            "1. Online Courses: There are many online courses available that can teach you the fundamentals of MLOps. Platforms like Coursera, Udemy, and edX offer courses specifically focused on MLOps. These courses usually cover topics such as deploying machine learning models, managing data pipelines, and monitoring model performance.\n",
            "\n",
            "2. Books and Blogs: There are several books and blogs written by experts in the field of MLOps that can provide you with valuable insights. Some popular books include \"Building Machine Learning Powered Applications\" by Emmanuel Ameisen and \"MLOps: Continuous Delivery and Automation Pipelines in Machine Learning\" by Mark Treveil. Blogs like Towards Data Science and Medium also have a wealth of articles on MLOps.\n",
            "\n",
            "3. MLOps Community: Engaging with the MLOps community can be a great way to learn from others and stay updated on the latest trends. Joining online communities like the MLOps community Slack or participating in MLOps meetups can provide you with opportunities to connect with like-minded individuals, ask questions, and share knowledge.\n",
            "\n",
            "4. Hands-on Projects: One of the best ways to learn MLOps is by working on real-world projects. Start by building a simple machine learning model and then focus on deploying it in a production environment. This will give you hands-on experience with tools and techniques used in MLOps, such as containerization, version control, and continuous integration/continuous deployment (CI/CD) pipelines.\n",
            "\n",
            "Remember, learning MLOps is an ongoing process, and it's important to stay curious and keep exploring new ideas and technologies. Good luck on your MLOps journey!\n",
            "{'question': 'How can I learn about MLOps?', 'answer': 'If you\\'re interested in learning about MLOps, there are several ways you can get started. Here are a few suggestions:\\n\\n1. Online Courses: There are many online courses available that can teach you the fundamentals of MLOps. Platforms like Coursera, Udemy, and edX offer courses specifically focused on MLOps. These courses usually cover topics such as deploying machine learning models, managing data pipelines, and monitoring model performance.\\n\\n2. Books and Blogs: There are several books and blogs written by experts in the field of MLOps that can provide you with valuable insights. Some popular books include \"Building Machine Learning Powered Applications\" by Emmanuel Ameisen and \"MLOps: Continuous Delivery and Automation Pipelines in Machine Learning\" by Mark Treveil. Blogs like Towards Data Science and Medium also have a wealth of articles on MLOps.\\n\\n3. MLOps Community: Engaging with the MLOps community can be a great way to learn from others and stay updated on the latest trends. Joining online communities like the MLOps community Slack or participating in MLOps meetups can provide you with opportunities to connect with like-minded individuals, ask questions, and share knowledge.\\n\\n4. Hands-on Projects: One of the best ways to learn MLOps is by working on real-world projects. Start by building a simple machine learning model and then focus on deploying it in a production environment. This will give you hands-on experience with tools and techniques used in MLOps, such as containerization, version control, and continuous integration/continuous deployment (CI/CD) pipelines.\\n\\nRemember, learning MLOps is an ongoing process, and it\\'s important to stay curious and keep exploring new ideas and technologies. Good luck on your MLOps journey!', 'sources': '', 'source_documents': [Document(page_content=\"So that's kind of the basics of MLOps.\", metadata={'source': 821, 'source_url': 'E9D3yg0kpQg', 'title': 'The Emerging Toolkit for Reliable, High-quality LLM Applications // Matei Zaharia //LLMs in Prod Con', 'author': 'MLOps.community'}), Document(page_content=\"So that's kind of the basics of MLOps.\", metadata={'source': 1083, 'source_url': '3XNUxwk1M6Y', 'title': 'LLMs in Production Conference - Part II', 'author': 'MLOps.community'}), Document(page_content='in the MLOps community,', metadata={'source': 1859, 'source_url': '1_NTxx3CJXg', 'title': 'Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk', 'author': 'MLOps.community'}), Document(page_content='MLOps community Slack,', metadata={'source': 1862, 'source_url': 'WQJdjRyYPwY', 'title': '$360k Question - Understanding the LLM Economics // Nikunj Bajaj // LLMs in Production Conference', 'author': 'MLOps.community'})]}\n",
            "\n",
            "Source 1:\n",
            "Video title: The Emerging Toolkit for Reliable, High-quality LLM Applications // Matei Zaharia //LLMs in Prod Con\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/E9D3yg0kpQg?t=821\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 2:\n",
            "Video title: LLMs in Production Conference - Part II\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/3XNUxwk1M6Y?t=1083\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 3:\n",
            "Video title: Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/1_NTxx3CJXg?t=1859\n",
            "Content: in the MLOps community,\n",
            "\n",
            "Source 4:\n",
            "Video title: $360k Question - Understanding the LLM Economics // Nikunj Bajaj // LLMs in Production Conference\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/WQJdjRyYPwY?t=1862\n",
            "Content: MLOps community Slack,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "032a47f8"
      },
      "outputs": [],
      "source": [
        "query = \"what are the key components of MLOps?\"\n",
        "result = chain(query)"
      ],
      "id": "032a47f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcrUA39A86G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0651bcd0-00b9-467f-d333-d0727b138339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what are the key components of MLOps?\n",
            "The key components of MLOps are:\n",
            "\n",
            "1. Data Management: This involves collecting, storing, and organizing the data that is used to train and test machine learning models. Good data management ensures that the data is accurate, complete, and properly labeled.\n",
            "\n",
            "2. Model Development: This is the process of creating and fine-tuning machine learning models. It involves selecting the right algorithms, training the models on the data, and evaluating their performance.\n",
            "\n",
            "3. Model Deployment: Once a machine learning model is developed, it needs to be deployed into a production environment where it can be used to make predictions or automate tasks. This involves setting up the necessary infrastructure, such as servers or cloud platforms, and integrating the model into existing systems.\n",
            "\n",
            "4. Monitoring and Maintenance: After a model is deployed, it needs to be continuously monitored to ensure that it is performing as expected. This includes monitoring its accuracy, detecting any drift or degradation in performance, and making necessary updates or retraining the model when needed.\n",
            "\n",
            "5. Automation and Orchestration: MLOps involves automating and orchestrating the different stages of the machine learning lifecycle. This includes automating data preprocessing, model training, deployment, and monitoring processes to ensure efficiency and consistency.\n",
            "\n",
            "6. Collaboration and Communication: MLOps encourages collaboration and communication between different teams involved in the machine learning lifecycle, such as data scientists, engineers, and operations teams. This helps in sharing knowledge, resolving issues, and ensuring smooth coordination throughout the process.\n",
            "\n",
            "These components work together to create a streamlined and efficient process for developing, deploying, and maintaining machine learning models in production. By following these practices, organizations can ensure that their machine learning projects are successful and deliver value. (\n",
            "\n",
            "Source 1:\n",
            "Video title: The Emerging Toolkit for Reliable, High-quality LLM Applications // Matei Zaharia //LLMs in Prod Con\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/E9D3yg0kpQg?t=821\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 2:\n",
            "Video title: LLMs in Production Conference - Part II\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/3XNUxwk1M6Y?t=1083\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 3:\n",
            "Video title: Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/1_NTxx3CJXg?t=1859\n",
            "Content: in the MLOps community,\n",
            "\n",
            "Source 4:\n",
            "Video title: Large Language Models in Production Round-table Conversation\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/rpjLTHrl-S4?t=246\n",
            "Content: which was one of the first MLOps companies.\n"
          ]
        }
      ],
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "id": "AcrUA39A86G6"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who leads the MLOps Community?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "0bo2rD9cK2gm"
      },
      "id": "0bo2rD9cK2gm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0DidsAPLBX3",
        "outputId": "c4fa21e9-4a36-4269-b58a-217e4372cc64"
      },
      "id": "x0DidsAPLBX3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who leads the MLOps Community?\n",
            "The MLOps Community is a collaborative community, so there isn't a single person who leads it. Instead, it is led by a group of passionate individuals who are experts in the field of MLOps. They work together to organize events, share knowledge, and facilitate discussions within the community. You can find these leaders actively participating in various platforms such as the MLOps Community Slack, where they help answer questions and provide guidance to members. They also contribute to the MLOps Community podcast, where they share insights and interview industry experts. So, it's a collective effort by many dedicated individuals who are passionate about advancing MLOps practices. (\n",
            "\n",
            "Source 1:\n",
            "Video title: Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/1_NTxx3CJXg?t=1859\n",
            "Content: in the MLOps community,\n",
            "\n",
            "Source 2:\n",
            "Video title: $360k Question - Understanding the LLM Economics // Nikunj Bajaj // LLMs in Production Conference\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/WQJdjRyYPwY?t=1862\n",
            "Content: MLOps community Slack,\n",
            "\n",
            "Source 3:\n",
            "Video title: Build and Customize LLMs in Less than 10 Lines of YAML // Travis Addair // LLMs in Prod Con Part 2\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/De6RY2GN-e4?t=102\n",
            "Content: on the MLOps community podcast.\n",
            "\n",
            "Source 4:\n",
            "Video title: Building LLM Applications for Production // Chip Huyen // LLMs in Prod Conference\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/spamOhG7BOA?t=2075\n",
            "Content: MLOps community slack,\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
