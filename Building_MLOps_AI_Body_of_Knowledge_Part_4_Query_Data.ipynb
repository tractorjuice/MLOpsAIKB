{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/MLOpsAIKB/blob/main/Building_MLOps_AI_Body_of_Knowledge_Part_4_Query_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# MLOps AI Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 4, query the vector database using ChatGPT\n",
        "\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This does not require a GPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "JSzQ8Ud4WeiH"
      },
      "id": "JSzQ8Ud4WeiH"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8Uynu3LfWYst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2c7002-8947-42db-f62d-b85db236a1fd"
      },
      "id": "8Uynu3LfWYst",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "KB_FOLDER = \"/content/gdrive/MyDrive/MLOpsKB\"  # Google drive folder to save the knowledgebase\n",
        "YT_DATASTORE = os.path.join(KB_FOLDER, \"youtube/datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "YT_AUDIO_FOLDER = os.path.join(KB_FOLDER, \"youtube/audio\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_FOLDER = os.path.join(YT_AUDIO_FOLDER, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "TRANSCRIPTS_TEXT_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"text\")  # Sub-directory for text of audio files\n",
        "TRANSCRIPTS_WHISPER_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(KB_FOLDER):\n",
        "    os.makedirs(KB_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(YT_DATASTORE):\n",
        "    os.makedirs(YT_DATASTORE)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(YT_AUDIO_FOLDER):\n",
        "    os.makedirs(YT_AUDIO_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_TEXT_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_TEXT_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_WHISPER_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_WHISPER_FOLDER)"
      ],
      "metadata": {
        "id": "vWeQuyzjU0m_"
      },
      "id": "vWeQuyzjU0m_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load required dependencies"
      ],
      "metadata": {
        "id": "C2dzEuzz2_di"
      },
      "id": "C2dzEuzz2_di"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6TkUqCJB_2X"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q tiktoken"
      ],
      "id": "W6TkUqCJB_2X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03__ie72B8J_"
      },
      "source": [
        "Use Pinecone or FAISS for the Vector Database"
      ],
      "id": "03__ie72B8J_"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = 'FAISS' # Set to 'Pinecone' or 'FAISS' for the vector datbase"
      ],
      "metadata": {
        "id": "cckWJ2E2TAjK"
      },
      "id": "cckWJ2E2TAjK",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    !pip install -q pinecone-client\n",
        "    from langchain.vectorstores import Pinecone\n",
        "    from tqdm.autonotebook import tqdm\n",
        "    import pinecone\n",
        "\n",
        "    # initialize pinecone\n",
        "    pinecone.init(\n",
        "        api_key=\"\",  # find at app.pinecone.io\n",
        "        environment=\"us-west4-gcp-free\"  # next to api key in console\n",
        "        )\n",
        "\n",
        "    index_name = \"knowledge\" # Put your Pincecone index name here\n",
        "    name_space = \"mlopskb\" # Put your Pincecone namespace here\n",
        "\n",
        "else:\n",
        "    !pip install -q faiss-cpu\n",
        "    from langchain.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "YoCtnnwMS9V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3917809-7e05-4d09-a44c-34b4898bb948"
      },
      "id": "YoCtnnwMS9V9",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUbD-eXCXNy-"
      },
      "source": [
        "Set up OPEN_API_KEY and necessary variables"
      ],
      "id": "TUbD-eXCXNy-"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tY7CJZoh5cma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # Add you OpenAI Key here\n",
        "\n",
        "#MODEL = \"gpt-3\"\n",
        "#MODEL = \"gpt-3.5-turbo\"\n",
        "#MODEL = \"gpt-3.5-turbo-0613\"\n",
        "#MODEL = \"gpt-3.5-turbo-16k\"\n",
        "MODEL = \"gpt-3.5-turbo-16k-0613\"\n",
        "#MODEL = \"gpt-4\"\n",
        "#MODEL = \"gpt-4-0613\"\n",
        "#MODEL = \"gpt-4-32k-0613\""
      ],
      "id": "tY7CJZoh5cma"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "# Query using the vector store with ChatGPT integration"
      ],
      "id": "35f99145"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup access to the Pinecone or FAISS vector database"
      ],
      "metadata": {
        "id": "S5Vf831zQLJa"
      },
      "id": "S5Vf831zQLJa"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ibz6Tz_7iPeQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "id": "ibz6Tz_7iPeQ"
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    vector_store = Pinecone.from_existing_index(index_name, embeddings, namespace=name_space)\n",
        "\n",
        "else:\n",
        "    # Open datastore\n",
        "    from langchain.vectorstores import FAISS\n",
        "    if os.path.exists(f\"{YT_DATASTORE}\"):\n",
        "        vector_store = FAISS.load_local(\n",
        "            f\"{YT_DATASTORE}\",\n",
        "            OpenAIEmbeddings()\n",
        "            )\n",
        "    else:\n",
        "        print(f\"Missing files. Upload index.faiss and index.pkl files to data_store directory first\")\n"
      ],
      "metadata": {
        "id": "QQDKy5IRT98I"
      },
      "id": "QQDKy5IRT98I",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the prompt"
      ],
      "metadata": {
        "id": "WDcuSm-wQPC-"
      },
      "id": "WDcuSm-wQPC-"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"\n",
        "    You are MLOpsGPT a mlops specialist bot.\n",
        "    You use examples from MLOps in your answers.\n",
        "    Your language should be for an 12 year old to understand.\n",
        "    If you do not know the answer to a question, do not make information up - instead, ask a follow-up question in order to gain more context.\n",
        "    Use a mix of technical and colloquial uk english language to create an accessible and engaging tone.\n",
        "    Use the following pieces of context to answer the users question.\n",
        "    Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
        "----------------\n",
        "{summaries}\n",
        "\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "id": "32a49412"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the LLM API"
      ],
      "metadata": {
        "id": "jlIfoZccQRmm"
      },
      "id": "jlIfoZccQRmm"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3018f865"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "llm = ChatOpenAI(model_name=MODEL, temperature=0)  # Modify model_name if you have access to GPT-4\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3}), # Use MMR search and return 5 (max 20) video sources\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "id": "3018f865"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "source": [
        "#### Use the chain to query"
      ],
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the sources so we can find the YouTube videos"
      ],
      "metadata": {
        "id": "6_aZT0JXPr_9"
      },
      "id": "6_aZT0JXPr_9"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I learn about MLOps?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "dRFI-plXIlPY"
      },
      "id": "dRFI-plXIlPY",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "8b1IKUV4IuZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13732678-1b05-4dc2-da61-e3942ec8b865"
      },
      "id": "8b1IKUV4IuZQ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I learn about MLOps?\n",
            "If you want to learn about MLOps, there are a few ways you can go about it. One option is to do some online research. You can search for articles, tutorials, and videos that explain what MLOps is and how it works. Another option is to join online communities or forums where people discuss MLOps. This can be a great way to ask questions and learn from others who are already experienced in the field.\n",
            "\n",
            "There are also dedicated websites and platforms that offer courses and training on MLOps. These courses can provide you with a structured learning path and hands-on experience. Some popular platforms for learning MLOps include Coursera, Udemy, and edX.\n",
            "\n",
            "Additionally, attending conferences, webinars, and workshops related to MLOps can be a valuable learning experience. These events often feature talks and presentations from experts in the field, giving you the opportunity to learn from their knowledge and experiences.\n",
            "\n",
            "Remember, learning about MLOps is an ongoing process, as the field is constantly evolving. So, it's important to stay curious, keep exploring, and stay updated with the latest developments in MLOps.\n",
            "\n",
            "\n",
            "\n",
            "Source 1:\n",
            "Video title: LLMs in Production Conference - Part II\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/3XNUxwk1M6Y?t=1083\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 2:\n",
            "Video title: Controlled and Compliant AI Applications // Daniel Whitenack // LLMs in Production Conference Part 2\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/xPwGyWugyVs?t=1373\n",
            "Content: out in the event link and on MLOps Slack if you want to reach out directly.\n",
            "\n",
            "Source 3:\n",
            "Video title: The Next Million AI Apps // Mark Huang // LLMs in Pod Con Part 2 Workshop Day 1\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/P68tSuuc010?t=3773\n",
            "Content: Just type in MLOps.community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "032a47f8"
      },
      "outputs": [],
      "source": [
        "query = \"what are the key components of MLOps?\"\n",
        "result = chain(query)"
      ],
      "id": "032a47f8"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AcrUA39A86G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cf31ff-1b1b-4907-ce8f-f9013910e40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what are the key components of MLOps?\n",
            "The key components of MLOps are:\n",
            "\n",
            "1. Data Management: This involves collecting, storing, and organizing the data that is used to train and test machine learning models. Good data management ensures that the data is accurate, complete, and properly labeled.\n",
            "\n",
            "2. Model Development: This is the process of creating and fine-tuning machine learning models. It involves selecting the right algorithms, training the models on the data, and evaluating their performance.\n",
            "\n",
            "3. Model Deployment: Once a model is developed, it needs to be deployed so that it can be used in real-world applications. This involves setting up the necessary infrastructure, such as servers or cloud platforms, and integrating the model into the existing software systems.\n",
            "\n",
            "4. Monitoring and Maintenance: After a model is deployed, it needs to be continuously monitored to ensure that it is performing as expected. This includes monitoring its accuracy, detecting any drift or degradation in performance, and making necessary updates or improvements.\n",
            "\n",
            "5. Automation: MLOps aims to automate as much of the machine learning lifecycle as possible. This includes automating tasks such as data preprocessing, model training, deployment, and monitoring. Automation helps to reduce human error, improve efficiency, and enable faster iteration and deployment of models.\n",
            "\n",
            "These components work together to create a smooth and efficient process for developing, deploying, and maintaining machine learning models. By following these practices, MLOps helps to ensure that machine learning models are reliable, scalable, and can be easily integrated into real-world applications. (\n",
            "\n",
            "Source 1:\n",
            "Video title: The Emerging Toolkit for Reliable, High-quality LLM Applications // Matei Zaharia //LLMs in Prod Con\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/E9D3yg0kpQg?t=821\n",
            "Content: So that's kind of the basics of MLOps.\n",
            "\n",
            "Source 2:\n",
            "Video title: Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/1_NTxx3CJXg?t=1859\n",
            "Content: in the MLOps community,\n",
            "\n",
            "Source 3:\n",
            "Video title: Reasoning Machines // Justin Uberti & Jon Turow // LLMs in Production Conference Lightning Talk 1\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/xchOwCMDLgE?t=17\n",
            "Content: useful and differentiated apps with LLMs by thinking through the stack, how you can do that from an MLOps perspective.\n"
          ]
        }
      ],
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "id": "AcrUA39A86G6"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who leads the MLOps Community?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "0bo2rD9cK2gm"
      },
      "id": "0bo2rD9cK2gm",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_url']}?t={int(document.metadata['source'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "x0DidsAPLBX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55113ba8-436a-49f8-eafd-960e069b6f24"
      },
      "id": "x0DidsAPLBX3",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who leads the MLOps Community?\n",
            "The MLOps Community is led by a group of people who are passionate about MLOps and its development. One of the key figures in the MLOps Community is the co-host of the MLOps Community podcast. She has been very active in the MLOps community and is dedicated to sharing knowledge and fostering collaboration among MLOps practitioners. While she is not the sole leader, she plays an important role in driving the community forward. (\n",
            "\n",
            "Source 1:\n",
            "Video title: The Next Million AI Apps // Mark Huang // LLMs in Pod Con Part 2 Workshop Day 1\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/P68tSuuc010?t=3771\n",
            "Content: in the MLOps community.\n",
            "\n",
            "Source 2:\n",
            "Video title: Evaluation // Panel 1 // Large Language Models in Production Conference Part 2\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/e0ZLqfus_TY?t=30\n",
            "Content: co-hosts on the MLOps community podcast.\n",
            "\n",
            "Source 3:\n",
            "Video title: Evaluation // Panel 1 // Large Language Models in Production Conference Part 2\n",
            "Video author: MLOps.community\n",
            "Source video: https://youtu.be/e0ZLqfus_TY?t=228\n",
            "Content: She's been very active in the MLOps\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}