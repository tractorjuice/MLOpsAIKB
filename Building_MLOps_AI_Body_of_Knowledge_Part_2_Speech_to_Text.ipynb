{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/MLOpsAIKB/blob/main/Building_MLOps_AI_Body_of_Knowledge_Part_2_Speech_to_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# MLOps Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 2, transcribe the audio files\n",
        "\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This requires a GPU/TPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZMGuOEMrG2V"
      },
      "source": [
        "## Runtime Checks"
      ],
      "id": "pZMGuOEMrG2V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAdXR7juCN_5"
      },
      "source": [
        "Check we are running on a GPU and check the available memory"
      ],
      "id": "DAdXR7juCN_5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95p5xdwLUSC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  gpu_info = !nvidia-smi\n",
        "except:\n",
        "  print('No GPU')\n",
        "else:\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  print(gpu_info)"
      ],
      "id": "b95p5xdwLUSC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi64puI8Lf--"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "Hi64puI8Lf--"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3l-ZazUZPR2"
      },
      "source": [
        "###Mount Google Drive"
      ],
      "id": "K3l-ZazUZPR2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trki7a-jZNzf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "trki7a-jZNzf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwk0Vu46YWjQ"
      },
      "source": [
        "Check files are stored on Google Drive"
      ],
      "id": "Kwk0Vu46YWjQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze4wgstKYprq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "KB_FOLDER = \"/content/gdrive/MyDrive/MLOpsKB\"  # Google drive folder to save the knowledgebase\n",
        "YT_DATASTORE = os.path.join(KB_FOLDER, \"youtube/datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "YT_AUDIO_FOLDER = os.path.join(KB_FOLDER, \"youtube/audio\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_FOLDER = os.path.join(YT_AUDIO_FOLDER, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "TRANSCRIPTS_TEXT_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"text\")  # Sub-directory for text of audio files\n",
        "TRANSCRIPTS_WHISPER_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(KB_FOLDER):\n",
        "    os.makedirs(KB_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(YT_DATASTORE):\n",
        "    os.makedirs(YT_DATASTORE)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(YT_AUDIO_FOLDER):\n",
        "    os.makedirs(YT_AUDIO_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_TEXT_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_TEXT_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_WHISPER_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_WHISPER_FOLDER)"
      ],
      "id": "ze4wgstKYprq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mHZRgDKXv1r"
      },
      "source": [
        "# Part 2 - Transcribe the audio files\n",
        "*(Skip to next section to load data store from files if it has been saved locally to save cost of embeddings)*"
      ],
      "id": "1mHZRgDKXv1r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKeWNvQxaqgz"
      },
      "source": [
        "## Use OpenAI Whisper to convert to text"
      ],
      "id": "kKeWNvQxaqgz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkLSiwIcw-lc"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "id": "xkLSiwIcw-lc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtnweVQDxTK6"
      },
      "outputs": [],
      "source": [
        "!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install --quiet git+https://github.com/sanchit-gandhi/whisper-jax.git datasets soundfile librosa\n",
        "!pip install --quiet cached_property"
      ],
      "id": "wtnweVQDxTK6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78y_hDZuxxpZ"
      },
      "outputs": [],
      "source": [
        "from whisper_jax import FlaxWhisperPipline\n",
        "import jax.numpy as jnp\n",
        "\n",
        "#For most GPUs, the dtype should be set to jnp.float16. For A100 GPUs or TPUs, the dtype should be set to jnp.bfloat16:\n",
        "pipeline = FlaxWhisperPipline(\"openai/whisper-large-v2\", dtype=jnp.bfloat16, batch_size=16)"
      ],
      "id": "78y_hDZuxxpZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2sy_Dxux4S6"
      },
      "outputs": [],
      "source": [
        "from jax.experimental.compilation_cache import compilation_cache as cc\n",
        "\n",
        "cc.initialize_cache(\"./jax_cache\")"
      ],
      "id": "w2sy_Dxux4S6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amQBmiBo_Ana"
      },
      "outputs": [],
      "source": [
        "unique_video_ids = []\n",
        "\n",
        "with open(f'{YT_AUDIO_FOLDER}/videos.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        # Remove linebreak which is the last character of the string\n",
        "        curr_place = line[:-1]\n",
        "        # Add item to the list\n",
        "        unique_video_ids.append(curr_place)"
      ],
      "id": "amQBmiBo_Ana"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhB97w1N_5oY"
      },
      "source": [
        "## Transcribe the audio files and save the text to the Google drive"
      ],
      "id": "vhB97w1N_5oY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ9YunCBR0fk"
      },
      "outputs": [],
      "source": [
        "import re, json, os\n",
        "\n",
        "def transcribe_file(filename):\n",
        "    print (f\"Transcribing New file: {filename}\")\n",
        "    transcription = pipeline(filename, return_timestamps=True)\n",
        "    print(transcription)\n",
        "    return transcription\n",
        "\n",
        "transcriptions = []\n",
        "total_videos = len(unique_video_ids)\n",
        "\n",
        "for counter, video in enumerate(unique_video_ids, start=1):\n",
        "    transcript_filename = f'{TRANSCRIPTS_WHISPER_FOLDER}/{video}_large.txt'\n",
        "    audio_filename = f'{YT_AUDIO_FOLDER}/clips/{video}.webm'\n",
        "    print(f\"{counter} of {total_videos}\")\n",
        "\n",
        "    if not os.path.isfile(transcript_filename):\n",
        "        if os.path.isfile(audio_filename):\n",
        "            transcription = transcribe_file(audio_filename)\n",
        "            transcriptions.append(transcription)\n",
        "            with open(transcript_filename, 'w') as f:\n",
        "                f.write(json.dumps(transcription))\n",
        "        else:\n",
        "            print (f\"File does not exist: {audio_filename}\")\n",
        "    else:\n",
        "        print(f\"Existing File: {transcript_filename}\")\n",
        "        with open(transcript_filename, 'r') as f:\n",
        "            transcription = json.load(f)\n",
        "        print(transcription)\n",
        "        with open(transcript_filename, 'w') as f:\n",
        "            f.write(json.dumps(transcription))\n"
      ],
      "id": "kQ9YunCBR0fk"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pZMGuOEMrG2V",
        "0UqlAAxTXnGF",
        "kKeWNvQxaqgz"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}